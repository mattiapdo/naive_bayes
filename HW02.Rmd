---
title: "Homework 2"
author: "Mattia Podio, Konstantine Christodoulou Sioumalas, Andrea Di Luca"
date: "24 maggio 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Load Data and Look at Them

We will perform analysis on data from sensors placed as 5 different units on the body 

(*T* Torso, 
*RA* Rigth Arm,
*LA* Left Arm,
*RL* Right Leg,
*LL* Left Leg) 

of a person, while he was performing several activities.

The activities are: 'walking', 'stepper', 'crosstr' and 'jumping', and there are 7500 observations for each class...

```{r}
load(file = 'daily-sport.RData')
table(dailysport$id)
```

Each sensor measure a specific quantity in the tree dimesional space, in particular:

- acceleration (x,y,z)

- radial acceleration (x,y,z)

- magnetic field (x,y,z)

Each feature is numerical, and there are no NAs:

```{r}
str(dailysport)
```

Let's reduce the dataset to two activities and drop the unused levels ('jumping' and 'walking').

This dataset is called 'ds.small', and contains just 3 sensors on one location
 
```{r}
ds.small = subset(x = dailysport, subset = id == "crosstr" | id == "stepper", select = c("id","RL-xMag","RL-yMag","RL-zMag"))
table(ds.small$id)

# drop unused levels:
ds.small = droplevels(ds.small)
table(ds.small$id)

colnames(ds.small) = c("id", "RLxMag", "RLyMag", "RLzMag")
str(ds.small)
```

```{r, message=F}
library(plotly)
```

Plot 500 points in ds.small choosen at random, treating them as 3-dimensional data points

```{r}
p <- plot_ly(ds.small[sample(x = 1:nrow(ds.small), size = 500, replace = F) ,], 
             x = ~RLxMag, y = ~RLyMag, z = ~RLzMag, color = ~id, colors = c('lightcoral', 'lightskyblue3')) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'RL-xMag'),
                     yaxis = list(title = 'RL-yMag'),
                     zaxis = list(title = 'RL-zMag')))
p
```

## Split training and test set

Split ds.small in ds.train and ds.test containing 70% and 30% of the original dataset respectively

```{r, message=F}
library(caret)
```

Let's use at this purpose the function createDataPartition() from caret package.

```{r}
partition = createDataPartition(y = ds.small$id, times = 1, p = 0.7, list = F)
ds.train = ds.small[partition,]
ds.test = ds.small[-partition,]
```

## Use the function lda() in the MASS package on ds.train to estimate a linear discriminant analysis model

```{r, message= F}
library(MASS) # for lda()
library(vegan) # for decostand()
library(dplyr)
```

```{r}
stepper = subset(ds.train,subset = id =="stepper")
cross =  subset(ds.train,subset = id =="crosstr")
#summary(stepper)
#hist(stepper$RLxMag)

num_attr = c("RLxMag", "RLyMag", "RLzMag")
num_var_st = stepper[,num_attr]
num_var_cr = cross[,num_attr]

#Standardize concerning the stepper
num_stand_step = decostand(num_var_st, "standardize")
colnames(num_stand_step) =  c("RLxMag", "RLyMag", "RLzMag")
#hist(num_stand_step$RLxMag)

#Standardize concerning the crosstr
num_stand_cross = decostand(num_var_cr, "standardize")
colnames(num_stand_cross) =  c("RLxMag", "RLyMag", "RLzMag")
#hist(num_stand_cross$RLyMag)

#Reobtain the data but now transformed
#for step
new.step=cbind(stepper,num_stand_step)
new_step = subset(new.step,select = c("id","RLxMag","RLyMag","RLzMag"))
#for cross
new.cross=cbind(cross,num_stand_cross)
new_cross = subset(new.cross,select = c("id","RLxMag","RLyMag","RLzMag"))

#Final

final = full_join(new_step,new_cross)

lda.out <- lda( formula = id ~ ., 
                data = final, 
                prior = c(1,1)/2)
lda.out
```

## Predict the class-labels on the training set

```{r}
pred.tr = predict(object = lda.out, ds.train)
table(predict = pred.tr$class, true  = ds.train$id)
```

## Predict the class-labels on the test set

```{r}
pred.te = predict(object = lda.out, ds.test)
table(predict = pred.te$class, true  = ds.test$id)
```

## Repeat everything using the function naivebayes() in the package e1071

```{r, message= F}
library(e1071)
```

Build the model

```{r}
na.bay = naiveBayes(id~., data = ds.train)
```

## Predict the class-labels on the train set

```{r}
pred.tr = predict(object = na.bay, ds.train)
table(predict = pred.tr, true  = ds.train$id)
```


## Predict the class-labels on the test set

```{r}
pred.te = predict(object = na.bay, ds.test)
table(predict = pred.te, true  = ds.test$id)
```


## Estimate the 1-dimensional class-conditional densities of each of the three covariates using histograms or kernels and build 95% bootstrap confidence bands around them

```{r, message=FALSE}
library(kedd) # h.amise
attach(ds.small)
```

Find the optimal bandwidth for the kernel estimations of the class conditional probability density functions $p_{stepper}(x_i) \quad p_{crosstr}(x_i) \quad i \in \{1,2,3\}$

```{r}
amise_cr_RLxMag = h.amise(ds.small$RLxMag[ds.small$id == "crosstr"])
amise_st_RLxMag = h.amise(ds.small$RLxMag[ds.small$id == "stepper"])
amise_cr_RLyMag = h.amise(ds.small$RLyMag[ds.small$id == "crosstr"])
amise_st_RLyMag = h.amise(ds.small$RLyMag[ds.small$id == "stepper"])
amise_cr_RLzMag = h.amise(ds.small$RLzMag[ds.small$id == "crosstr"])
amise_st_RLzMag = h.amise(ds.small$RLzMag[ds.small$id == "stepper"])

h_cr_RLxMag = amise_cr_RLxMag$h 
h_st_RLxMag = amise_st_RLxMag$h
h_cr_RLyMag = amise_cr_RLyMag$h 
h_st_RLyMag = amise_st_RLyMag$h
h_cr_RLzMag = amise_cr_RLzMag$h 
h_st_RLzMag = amise_st_RLzMag$h
```

Kernel densities estimation

```{r}
densities = list(
p_cr_RLxMag = density(ds.small$RLxMag[ds.small$id == "crosstr"], bw = h_cr_RLxMag),
p_st_RLxMag = density(ds.small$RLxMag[ds.small$id == "stepper"], bw = h_st_RLxMag),
p_cr_RLyMag = density(ds.small$RLyMag[ds.small$id == "crosstr"], bw = h_cr_RLyMag),
p_st_RLyMag = density(ds.small$RLyMag[ds.small$id == "stepper"], bw = h_st_RLyMag),
p_cr_RLzMag = density(ds.small$RLzMag[ds.small$id == "crosstr"], bw = h_cr_RLzMag),
p_st_RLzMag = density(ds.small$RLzMag[ds.small$id == "stepper"], bw = h_st_RLzMag)
)
```

```{r}
par(mfrow = c(3,1))
plot(densities[[1]] , main = "",
     xlim = c(min(ds.small$RLxMag),max(ds.small$RLxMag)),
     ylim = c(0,10), 
     xlab = "RLxMag",
     col = "lightblue3", lwd = 3)

lines(densities[[2]] , col = "lightcoral", lwd = 3)

legend("topright", c("stepper", "crosstr"), col = c("lightcoral", "lightblue3"),
       text.col = "green4", lwd = c(3, 3))

plot(densities[[3]] , main = "",
     xlim = c(min(ds.small$RLyMag),max(ds.small$RLyMag)),
     ylim = c(0,5), 
     xlab = "RLyMag",
     col = "lightblue3", lwd = 3)

lines(densities[[4]] , col = "lightcoral", lwd = 3)

legend("topright", c("stepper", "crosstr"), col = c("lightcoral", "lightblue3"),
       text.col = "green4", lwd = c(3, 3))

plot(densities[[5]] , main = "",
     xlim = c(min(ds.small$RLzMag),max(ds.small$RLzMag)),
     ylim = c(0,20), 
     xlab = "RLzMag",
     col = "lightblue3", lwd = 3)

lines(densities[[6]] , col = "lightcoral", lwd = 3)

legend("topleft", c("stepper", "crosstr"), col = c("lightcoral", "lightblue3"),
       text.col = "green4", lwd = c(3, 3))
```

### Bootstrap Confidence Bands

Here is the function to perform bootstrap in order to get confidence band for the density estimate.

```{r}
kern.bootstrap = function(data, B, h, alpha){
  n = length(data)
  samples = rep(NA,B)
  true.dens = density(x = data, bw = h)
  for (b in 1:B){
    indexes = sample(x = 1:length(data), size = n, replace = T)
    boot.sample = data[indexes]
    boot.dens = density(x = boot.sample, bw = h)
    boot.stat = sqrt(n*h)*max(abs(true.dens$y-boot.dens$y))
    samples[b] = boot.stat 
  }
  q = quantile(x = samples, probs = alpha)
  print(q)
  zero = rep(0, length(true.dens))
  return(list("x" = true.dens$x, 
              "p.hat" = true.dens$y, 
              "lo" =ifelse(zero>(true.dens$y - q /sqrt(n*h)),zero, true.dens$y - q /sqrt(n*h)),
              "up" = true.dens$y + q/sqrt(n*h)))
}
```

```{r}
out = kern.bootstrap(data = ds.small$RLxMag[ds.small$id == "crosstr"], B = 100, h = h_cr_RLxMag, alpha = 0.90)
plot(out$x, out$up, type = "l", col = "green", ylim = c(0,5.5))
points(out$x, out$p.hat, type = "l", col = "black", lwd = 2)
points(out$x, out$lo, type = "l", col = "red")
str(out)

out = kern.bootstrap(data = ds.small$RLxMag[ds.small$id == "crosstr"], B = 100, h = h_cr_RLxMag, alpha = 0.10)
plot(out$x, out$up, type = "l", col = "green", ylim = c(0,5.5))
points(out$x, out$p.hat, type = "l", col = "black", lwd = 2)
points(out$x, out$lo, type = "l", col = "red")
str(out)
```

### We implement our own version of the *naive bayes classifier*

$p_{crosstr}(\vec{x}) = \prod_{j=1}^3 p_{crosstr}(x_j)$
$p_{stepper}(\vec{x}) = \prod_{j=1}^3 p_{stepper}(x_j)$

```{r}
joint = function(x, den, i){
  # this function computes the joint probability density function
  # assuming independance above the variables. In other words,
  # it is assumed that the joint pdf factorizes into the product
  # of the one dimensional pdfs.
  
  # x is a vector
  # den is a list of functions - the 1-d density functions
  # i is an additional integer parameter: is the index of the 
  # class variable to be dropped
  
  
  x = x[-i]
  # init a vector that will contain the values of the 1d pdfs
  joint = rep(0, ncol(x))
  # for each feature in the vector, store the 1d pdf
  for(j in 1:ncol(x)){
    joint[j] = apply(x[j],1, den[[j]])
  }
  # return the product of the pdfs
  return(prod(joint))
}

# The r function computes the regression function
r = function(l1, l2, p1, p2) return(p1*l1/(p1*l1+p2*l2))


na.bay = function(test, train, i){
  # This function solves the binary classification using naive bayes algorithm
  # test is a dataframe with the new observations
  # data is the training set
  # i is the index of the label column in the training set
  
  # save the column with the true classes in the train set
  trueclass = train[i]
  
  # split the train set into two subsest according the class
  # the data belong to
  X <- split(train, train[i])
  
  # save the names of the two classes
  a = as.character(unique(X[[1]][,i]))
  b = as.character(unique(X[[2]][,i]))
  
  # compute the empirical probability density functions for the
  # two classes
  pi1 = nrow(X[[1]])/nrow(train)
  pi2 = nrow(X[[2]])/nrow(train)
  # estimate the conditional density functions and interpolate
  # them in order to evaluate them for new datapoints
  den1<-apply(X[[1]][,-i], 2, density)
  den2<-apply(X[[2]][,-i], 2, density)
  den1 = lapply(den1, approxfun, rule = 2)
  den2 = lapply(den2, approxfun, rule = 2)
  
  # split the test set into a list containig each separate rows
  # this step is necessary to use the lapply function
  list <- split(test, seq(nrow(test)))
  # for each item (datapoint) in the list, compute the joint
  # conditional probability density functions using the 
  # Naive Bayes rule.
  # ...see joint() function for more details
  f1 = unlist(lapply(list, joint, den1, i))
  f2 = unlist(lapply(list, joint, den2, i))
  
  # evaluate the regression statistic
  r = r(f1, f2, pi1, pi2)

  # for each row in the test set, use the value of the regression
  # function to establish to which class the datapoint belongs to
  Prediction = rep(NA, nrow(test))
  for (i in 1:nrow(test)){
    if(r[i]>1/2) {
      Prediction[i] <- a
      }
    else {
      Prediction[i] <- b
    }
  }
  return(factor(Prediction))
}

```

Toy example to - comparing performances.

```{r}
dataA = data.frame(x = rnorm(1000, -2, 1.5), y = rnorm(1000, 2, 1.5), class = "classA")
dataB = data.frame(x = rnorm(1000, 2, 1.5), y = rnorm(1000, -2, 1.5), class = "classB")
data = rbind(dataB, dataA)
plot(data$x, data$y, col = data$class, pch = 19, xlab = "x", ylab = "y", 
     main = "Scatterplot x vs y")

# Split Train set and Test set
intr = createDataPartition(data$class, p = 0.7, list = F)
train = data[intr,]
test = data[-intr,]

cat("- Using our function na.bay()...\n")
time = Sys.time()
predictions = na.bay(test = test, train = train, i = 3)
cat("\nAccuracy = ", round(sum(predictions == test$class)/length(predictions),4) ,"\n")
print(table(test$class, predictions, dnn = c("True", "Predicted")))
cat(paste("Elapsed time = ", round(Sys.time()-time, 5), "s\n"))

cat("\n-Using the function naiveBayes() from e1071...\n")
time = Sys.time()
model = naiveBayes(class~., data = train)
pred.tr = predict(object = model, newdata = test)
table = table(test$class, pred.tr, dnn = c("True", "Predicted"))
cat(paste("\nAccuracy = ", round(sum(diag(table))/sum(table), 4)),"\n")
print(table)
cat(paste("Elapsed time = ", round(Sys.time()-time, 5), "s\n"))
```

In terms of accuracy the classifiers are similar, while our implementation takes twice the time of the naiveBayes() function.

## Other stuff

Identifying Correlated Predictors and remove them

```{r}
colnames(dailysport)
predictors = subset(dailysport, select = -id)
# ncol(predictors)
# highlyCorDescr <- findCorrelation(cor(predictors), cutoff = .75)
# predictors <- predictors[,-highlyCorDescr]
# ncol(predictors)
# cor <- cor(predictors)
# summary(cor[upper.tri(cor)])
data = cbind(id = dailysport$id, predictors) 
```

### Linear Support Vector Machine

```{r}
inTrain = createDataPartition(y = data$id, p = 0.7, list = FALSE)

train = data[inTrain,]; cat("nrow train", nrow(train), "\n")
test = data[-inTrain,]; cat("nrow test", nrow(test), "\n")

```

```{r, echo = F, eval = F}
scale = preProcess(x = train)
train = predict(scale, train)

# Torso
featurePlot(x = train[,2:10],
            y= train$id,
            plot = "box",
            layout = c(3,3),
            scales = list(y = list(relation = "free"),
                          x = list(rot = 90))
            )
# R Arm
featurePlot(x = train[,11:19],
            y = train$id,
            plot = "box",
            layout = c(3,3),
            scales = list(y = list(relation = "free"),
                          x = list(rot = 90))
            
            )

# L Arm
featurePlot(x = train[,20:28],
            y = train$id,
            plot = "box",
            layout = c(3,3),
            scales = list(x = list(rot = 90),
                          y = list(relation = "free"))
            )

#R Leg 
featurePlot(x = train[,29:37],
            y = train$id,
            plot = "box",
            scales = list(x = list(rot = 90),
                          y = list(relation = "free")),
            layout = c(3,3)
            )

# L Leg 
featurePlot(x = train[,38:46],
            y = train$id,
            plot = "box",
            scales = list(x = list(rot = 90),
                          y = list(relation = "free")),
            layout = c(3,3)
            )
```

```{r}
# Feature Importance Recursive Feature Elimination

subsets = 1:5
ctrl = rfeControl(functions = rfFuncs,
                  method = "cv",
                  number = 3,
                  verbose = T
                  )
lmProfile = rfe(x = train[,2:10],
                y = train$id,
                sizes = subsets,
                rfeControl = ctrl
                )
lmProfile

lmProfile$fit
```


```{r}
# Control the training with 3 fold cross validation
ctrl = trainControl(method = "cv", 
                    number = 3,
                    verboseIter = T)

grid = expand.grid(C = c(1,2,3,4))
svm_Linear = train(x = train[,2:10],
                   y = train$id,
                   method = "svmLinear",
                   trControl = ctrl,
                   tuneGrid = grid
                   )
svm_Linear
```

```{r}
inTrain = createDataPartition(y = data$id, p = 0.7, list = FALSE)

train = data[inTrain,]; cat("nrow train", nrow(train), "\n")
test = data[-inTrain,]; cat("nrow test", nrow(test), "\n")

# Control the training with 3 fold cross validation
trctrl = trainControl(method = "none", number = 3)
? train
grid = expand.grid(C= seq(0.1, 1, 0.1))
LDA = train(id ~., data = train, method = "lda",
                   trControl = trctrl,
                   #tuneGrid = grid,
                   #preProcess = c("center", "scale"),
                   tunelength = 10)
LDA

pred= predict(LDA, newdata = test)
confusionMatrix(pred, test$id)
```

### Take one Sensor each time

```{r}
data = dailysport
inTrain = createDataPartition(y = data$id, p = 0.7, list = FALSE)

for (i in seq(2,8,3))  {
  train = data[inTrain,c(1, i, i+1, i+2)]; 
  test = data[-inTrain,c(1, i, i+1, i+2)]; 

  # Control the training with 3 fold cross validation
  trctrl = trainControl(method = "none", number = 3)
  grid = expand.grid(C= seq(0.1, 1, 0.1))
  LDA = train(id ~., data = train, method = "lda",
                     trControl = trctrl,
                     #tuneGrid = grid,
                     #preProcess = c("center", "scale"),
                     tunelength = 10)
  print(LDA)
  

  pred= predict(LDA, newdata = test)
  print(confusionMatrix(pred, test$id))
  print("\n\n***************************************************\n\n" )
 }
```


