---
title: "Homework 2"
author: "Mattia Podio"
date: "24 maggio 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#rm(list = ls())
```

### Load Data and Look at Them

We will perform analysis on data from sensors placed as 5 different units on the body 

(*T* Torso, 
*RA* Rigth Arm,
*LA* Left Arm,
*RL* Right Leg,
*LL* Left Leg) 

of a person, while he was performing several activities.

```{r}
load(file = 'daily-sport.RData')
```

The activities are: 'walking', 'stepper', 'crosstr' and 'jumping', and there are 7500 observations for each class...

```{r}
table(dailysport$id)
```

Each sensor measure a specific quantity in the tree dimesional space, in particular:

- acceleration (x,y,z)

- radial acceleration (x,y,z)

- magnetic field (x,y,z)

Each feature is numerical, and there are no NAs:

```{r}
str(dailysport)
```

```{r}
cat('NAs:',sum(is.na(dailysport)))
```

Let's reduce the dataset to two activities and drop the unused levels ('jumping' and 'walking').

This dataset is called 'ds.small', and contains just 3 sensors on one location
 
```{r}
ds.small = subset(x = dailysport, subset = id == "crosstr" | id == "stepper", select = c("id","RL-xMag","RL-yMag","RL-zMag"))
table(ds.small$id)

# drop unused levels:
ds.small = droplevels(ds.small)
table(ds.small$id)

colnames(ds.small) = c("id", "RLxMag", "RLyMag", "RLzMag")
str(ds.small)
```

```{r, echo = T, eval = T}
library(plotly)
```

Plot 500 points in ds.small choosen at random, treating them as 3-dimensional data points

```{r}
p <- plot_ly(ds.small[sample(x = 1:nrow(ds.small), size = 500, replace = F) ,], 
             x = ~RLxMag, y = ~RLyMag, z = ~RLzMag, color = ~id, colors = c('lightcoral', 'lightskyblue3')) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'RL-xMag'),
                     yaxis = list(title = 'RL-yMag'),
                     zaxis = list(title = 'RL-xMag')))
p
```

## Split training and test set

Split ds.small in ds.train and ds.test containing 70% and 30% of the original dataset respectively

```{r, eval=T}
library(caret)
```

```{r}
partition = createDataPartition(y = ds.small$id, times = 1, p = 0.7, list = F)
ds.train = ds.small[partition,]
ds.test = ds.small[-partition,]
cat(paste('nrow(ds.train) + nrow(ds.test):', nrow(ds.train) + nrow(ds.test), '\nnrow(ds.small):', nrow(ds.small)))
```

## Use the function lda() in the MASS package on ds.train to estimate a linear discriminant analysis model

```{r}
library(MASS) # for lda()
library(vegan) # for decostand()
library(dplyr)
```

```{r, eval = T, echo = T}

stepper = subset(ds.train,subset = id =="stepper")
cross =  subset(ds.train,subset = id =="crosstr")
#summary(stepper)
#hist(stepper$RLxMag)

num_attr = c("RLxMag", "RLyMag", "RLzMag")
num_var_st = stepper[,num_attr]
num_var_cr = cross[,num_attr]

#Standardize concerning the stepper
num_stand_step = decostand(num_var_st, "standardize")
colnames(num_stand_step) =  c("RLxMag", "RLyMag", "RLzMag")
#hist(num_stand_step$RLxMag)

#Standardize concerning the crosstr
num_stand_cross = decostand(num_var_cr, "standardize")
colnames(num_stand_cross) =  c("RLxMag", "RLyMag", "RLzMag")
#hist(num_stand_cross$RLyMag)

#Reobtain the data but now transformed
#for step
new.step=cbind(stepper,num_stand_step)
new_step = subset(new.step,select = c("id","RLxMag","RLyMag","RLzMag"))
#for cross
new.cross=cbind(cross,num_stand_cross)
new_cross = subset(new.cross,select = c("id","RLxMag","RLyMag","RLzMag"))

#Final

final = full_join(new_step,new_cross)

lda.out <- lda(formula = id ~ ., 
         data = final, 
         prior = c(1,1)/2)
lda.out
```

## Predict the class-labels on the training set

```{r}
pred.tr = predict(object = lda.out, ds.train)
table(predict = pred.tr$class, true  = ds.train$id)
```

## Predict the class-labels on the test set

```{r}
pred.te = predict(object = lda.out, ds.test)
table(predict = pred.te$class, true  = ds.test$id)
```

## Repeat everything using the function naivebayes() in the package e1071

```{r}
#library(e1071)
? naiveBayes()
```

Build the model

```{r}
na.bay = naiveBayes(id~., data = ds.train)
```

```{r}
names(na.bay)
```

"apriori" is the class distribution for the dependent variable

```{r}
na.bay$apriori
```

```{r}
names(na.bay$tables)
na.bay$tables$RLxMag
na.bay$tables$RLyMag
na.bay$tables$RLzMag
```

## Predict the class-labels on the train set

```{r}
pred.tr = predict(object = na.bay, ds.train)
table(predict = pred.tr, true  = ds.train$id)
```

## Predict the class-labels on the test set

```{r}
pred.te = predict(object = na.bay, ds.test)
table(predict = pred.te, true  = ds.test$id)
```

## Estimate the 1-dimensional class-conditional densities of each of the three covariates using histograms or kernels and build 95% bootstrap confidence bands around them

```{r}
head(ds.small)
nrow(ds.small)
```

```{r}
# install.packages("kedd")
library(kedd) # h.amise
attach(ds.small)
```

Find the optimal bandwidth for the kernel estimations of the class conditional probability density functions $p_{stepper}(x_i) \quad p_{crosstr}(x_i) \quad i \in \{1,2,3\}$

```{r}
amise_cr_RLxMag = h.amise(ds.small$RLxMag[ds.small$id == "crosstr"])
amise_st_RLxMag = h.amise(ds.small$RLxMag[ds.small$id == "stepper"])
amise_cr_RLyMag = h.amise(ds.small$RLyMag[ds.small$id == "crosstr"])
amise_st_RLyMag = h.amise(ds.small$RLyMag[ds.small$id == "stepper"])
amise_cr_RLzMag = h.amise(ds.small$RLzMag[ds.small$id == "crosstr"])
amise_st_RLzMag = h.amise(ds.small$RLzMag[ds.small$id == "stepper"])

h_cr_RLxMag = amise_cr_RLxMag$h 
h_st_RLxMag = amise_st_RLxMag$h
h_cr_RLyMag = amise_cr_RLyMag$h 
h_st_RLyMag = amise_st_RLyMag$h
h_cr_RLzMag = amise_cr_RLzMag$h 
h_st_RLzMag = amise_st_RLzMag$h
```

Kernel densities estimation

```{r}
densities = list(
p_cr_RLxMag = density(ds.small$RLxMag[ds.small$id == "crosstr"], bw = h_cr_RLxMag),
p_st_RLxMag = density(ds.small$RLxMag[ds.small$id == "stepper"], bw = h_st_RLxMag),
p_cr_RLyMag = density(ds.small$RLyMag[ds.small$id == "crosstr"], bw = h_cr_RLyMag),
p_st_RLyMag = density(ds.small$RLyMag[ds.small$id == "stepper"], bw = h_st_RLyMag),
p_cr_RLzMag = density(ds.small$RLzMag[ds.small$id == "crosstr"], bw = h_cr_RLzMag),
p_st_RLzMag = density(ds.small$RLzMag[ds.small$id == "stepper"], bw = h_st_RLzMag)
)
```


```{r}
plot(densities[[1]] , main = "",
     xlim = c(min(ds.small$RLxMag),max(ds.small$RLxMag)),
     ylim = c(0,10), 
     xlab = "RLxMag",
     col = "lightblue3", lwd = 3)

lines(densities[[2]] , col = "lightcoral", lwd = 3)

legend("topright", c("stepper", "crosstr"), col = c("lightcoral", "lightblue3"),
       text.col = "green4", lwd = c(3, 3))
```
```{r}
plot(densities[[3]] , main = "",
     xlim = c(min(ds.small$RLyMag),max(ds.small$RLyMag)),
     ylim = c(0,5), 
     xlab = "RLyMag",
     col = "lightblue3", lwd = 3)

lines(densities[[4]] , col = "lightcoral", lwd = 3)

legend("topright", c("stepper", "crosstr"), col = c("lightcoral", "lightblue3"),
       text.col = "green4", lwd = c(3, 3))

```
```{r}
plot(densities[[5]] , main = "",
     xlim = c(min(ds.small$RLzMag),max(ds.small$RLzMag)),
     ylim = c(0,20), 
     xlab = "RLzMag",
     col = "lightblue3", lwd = 3)

lines(densities[[6]] , col = "lightcoral", lwd = 3)

legend("topleft", c("stepper", "crosstr"), col = c("lightcoral", "lightblue3"),
       text.col = "green4", lwd = c(3, 3))

```

### We implement our own version of the *naive bayes classifier*

$p_{crosstr}(\vec{x}) = \prod_{j=1}^3 p_{crosstr}(x_j)$
$p_{stepper}(\vec{x}) = \prod_{j=1}^3 p_{stepper}(x_j)$

```{r}
naive_joint_cond = function(x,y,x){
  return()
}
```


